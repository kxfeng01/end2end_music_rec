
import json
import os
import time
import logging
import datetime
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json, to_timestamp
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType
from confluent_kafka import Consumer, KafkaError, KafkaException
import configparser

# ‚úÖ Setup Logging
LOG_DIR = "logs"
os.makedirs(LOG_DIR, exist_ok=True)
LOG_FILE = os.path.join(LOG_DIR, "consumer.log")

logging.basicConfig(
    filename=LOG_FILE,
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
)

# ‚úÖ Load Kafka Configuration from `client.properties`
def load_kafka_config(config_path="client.properties"):
    abs_path = os.path.abspath(config_path)
    if not os.path.exists(abs_path):
        raise FileNotFoundError(f"ERROR: {abs_path} not found!")

    config = configparser.ConfigParser()
    config.read(abs_path)

    section = "default"
    if section not in config:
        raise ValueError(f"ERROR: Missing section [{section}] in {config_path}")

    return {
        "bootstrap.servers": config.get(section, "bootstrap.servers"),
        "security.protocol": config.get(section, "security.protocol"),
        "sasl.mechanisms": config.get(section, "sasl.mechanisms"),
        "sasl.username": config.get(section, "sasl.username"),
        "sasl.password": config.get(section, "sasl.password"),
        "group.id": "music_rec_consumer",
        "auto.offset.reset": "earliest",  # ‚úÖ Start from latest messages
    }

# ‚úÖ Initialize Spark
spark = SparkSession.builder.appName("KafkaConsumer") \
    .config("spark.driver.memory", "8g") \
    .config("spark.executor.memory", "4g") \
    .getOrCreate()

# ‚úÖ Define Kafka Consumer
def create_consumer():
    kafka_config = load_kafka_config()
    return Consumer(kafka_config)

# ‚úÖ Define Schema for Incoming Data

schema = StructType([
    StructField("user_id", StringType(), True),
    StructField("song_id", StringType(), True),
    StructField("timestamp", StringType(), True),
    StructField("event_type", StringType(), True),
    StructField("plays", IntegerType(), True)  # ‚úÖ CORRECT COLUMN NAME
])


# ‚úÖ Function to Process Kafka Messages and Write Daily Parquet Files
def process_messages(messages, output_base_path="data/incremental"):
    if not messages:
        logging.info("No messages to process.")
        return

    parsed_data = []
    for msg in messages:
        try:
            record = json.loads(msg.value().decode("utf-8"))
            logging.info(f"Raw message: {record}")  # Log the raw message

            if record.get("event_type") == "play":
                logging.info(f"Processing 'play' event: {record}")  # Log the play event
                record["timestamp"] = datetime.fromisoformat(record["timestamp"])
                parsed_data.append(record)
            else:
                logging.info(f"Skipping non-'play' event: {record}")  # Log skipped events

        except (json.JSONDecodeError, ValueError) as e:
            logging.error(f"‚ùå JSON decode error: {e}")
            logging.error(f"‚ùå Faulty message: {msg.value().decode('utf-8')}")  # Log the problematic message

    if not parsed_data:
        logging.info("No 'play' events to process.")
        return

    # ‚úÖ Convert to Spark DataFrame
    df = spark.createDataFrame(parsed_data, schema=schema)

    # ‚úÖ Convert timestamp field to Spark TimestampType
    df = df.withColumn("timestamp", to_timestamp(col("timestamp")))

    # ‚úÖ Get today's date for folder structure
    today_date = datetime.datetime.now().strftime("%Y-%m-%d")
    output_path = os.path.join(output_base_path, today_date)
    os.makedirs(output_path, exist_ok=True)

    # ‚úÖ Append new data to daily Parquet folder
    parquet_path = os.path.join(output_path, "new_interactions.parquet")
    df.write.mode("append").parquet(parquet_path)

    logging.info(f"‚úÖ {len(parsed_data)} new interactions saved to {parquet_path}")
    print(f"‚úÖ {len(parsed_data)} new interactions saved to {parquet_path}")

# ‚úÖ Kafka Consumer Loop (Batch Processing Every 30 seconds)
def consume_data():
    consumer = create_consumer()
    consumer.subscribe(["daily_interactions"])
    
    logging.info("üéß Kafka Consumer started. Listening for new messages...")
    print("üéß Kafka Consumer started. Listening for new messages...")

    try:
        while True:
            msg_batch = []
            batch_start_time = time.time()

            logging.info("üîÑ Polling Kafka for new messages...")
            print("üîÑ Polling Kafka for new messages...")

            while time.time() - batch_start_time < 30:  # Poll for 30 seconds per batch
                print("üîç Attempting to poll Kafka...")
                msg = consumer.poll(timeout=1.0)

                if msg is None:
                    print("‚è≥ No messages received in this poll...")
                    continue

                if msg.error():
                    print(f"‚ùå Kafka error: {msg.error()}")
                    logging.error(f"‚ùå Kafka error: {msg.error()}")
                    if msg.error().code() == KafkaError._PARTITION_EOF:
                        continue
                    else:
                        raise KafkaException(msg.error())

                # ‚úÖ If we get here, we received a message
                message_value = msg.value().decode("utf-8")
                print(f"üì© Received message: {message_value}")
                logging.info(f"üì© Received message: {message_value}")
                
                msg_batch.append(msg)

                # ‚úÖ Process in smaller chunks for debugging
                if len(msg_batch) >= 5:
                    process_messages(msg_batch)
                    msg_batch = []

            if msg_batch:
                process_messages(msg_batch)

            logging.info("‚è≥ Sleeping for 30 seconds before checking again...")
            print("‚è≥ Sleeping for 30 seconds before checking again...")
            time.sleep(30)
    except KeyboardInterrupt:
        logging.info("üõë Consumer stopped manually.")
        print("üõë Consumer stopped manually.")
    finally:
        consumer.close()

# ‚úÖ Run Consumer
if __name__ == "__main__":
    consume_data()
